{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dcf29f",
   "metadata": {},
   "source": [
    "City Neighborhood Classifier \n",
    "\n",
    "CS 305: Machine Learning Fall 2021\n",
    "\n",
    "Sara Clark, Christine Pourheydarian, Jiawei Liu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1771af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: six>=1.10 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: urllib3 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.26.4)\n",
      "Requirement already satisfied: python-slugify in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: tqdm in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (4.59.0)\n",
      "Requirement already satisfied: certifi in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: requests in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.25.1)\n",
      "Requirement already satisfied: python-dateutil in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ef147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54351311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20170905' 0 None 12147973]\n",
      " ['20170904' 0 None 12147973]\n",
      " ['20170903' 0 None 12147973]\n",
      " ...\n",
      " ['20160908' 0 None 14504422]\n",
      " ['20160907' 0 None 14504422]\n",
      " ['20160906' 0 None 14504422]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "importing the calendar.csv data. \n",
    "listing id stays a number\n",
    "date is kept in its form but has the dashes removed\n",
    "available: if f, then becomes 0. if t, then becomes 1. \n",
    "price: converted from currency to a double. if there is no price, the value inserted is None. \n",
    "'''\n",
    "\n",
    "\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "calendar_csvfile = open('archive/calendar.csv', 'r')#listing_id,date,available,price\n",
    "reader = csv.reader(calendar_csvfile, delimiter=',', quotechar='\"')\n",
    "calendar_data = []\n",
    "\n",
    "for row in reader: \n",
    "    r  = [ ]\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            date = (row[1]).replace(\"-\", \"\")\n",
    "            r.append(date)\n",
    "        elif i == 1:\n",
    "            available = 0 if row[2] == \"f\" else 1 #f: 0, t: 1 \n",
    "            r.append(available)\n",
    "        elif i == 2:        \n",
    "            money = row[3]\n",
    "            if money == \"\":\n",
    "                r.append(None)\n",
    "                continue\n",
    "            else: \n",
    "                value = float(Decimal(sub(r'[^\\d.]', '', money)))#remove dollar sign, comma, and everything after the .\n",
    "                r.append(value)\n",
    "        elif i == 3: \n",
    "            listing_id  = int(row[0])\n",
    "            r.append(listing_id)    \n",
    "    calendar_data.append(r)\n",
    "\n",
    "\n",
    "calendar_data_np = np.array(calendar_data)\n",
    "print(calendar_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240f7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading reviews.cvs into numpy w/o any modification, i.e. w/o switching anything from qualitative to quantitative  \n",
    "'''\n",
    "\n",
    "csv_file = open('archive/reviews.csv', 'r')\n",
    "reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "data = []\n",
    "for i, row in enumerate(reader):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    r = []\n",
    "    for i in range(6):\n",
    "        r.append(row[i])\n",
    "    data.append(r)\n",
    "\n",
    "csv_file.close()\n",
    "reviews = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1129c6e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings data \n",
      " [['id' 'host_id' 'host_listings_count' 'host_total_listings_count'\n",
      "  'zipcode' 'latitude' 'longitude' 'is_location_exact' 'property_type'\n",
      "  'room_type' 'accommodates' 'bathrooms' 'bedrooms' 'beds' 'bed_type'\n",
      "  'square_feet' 'price' 'weekly_price' 'monthly_price' 'security_deposit'\n",
      "  'cleaning_fee' 'guests_included' 'extra_people' 'minimum_nights'\n",
      "  'maximum_nights' 'calendar_updated' 'availability_30' 'availability_60'\n",
      "  'availability_90' 'availability_365' 'number_of_reviews'\n",
      "  'review_scores_rating' 'review_scores_accuracy'\n",
      "  'review_scores_cleanliness' 'review_scores_checkin'\n",
      "  'review_scores_communication' 'review_scores_location'\n",
      "  'review_scores_value' 'requires_license' 'instant_bookable'\n",
      "  'cancellation_policy' 'require_guest_profile_picture'\n",
      "  'require_guest_phone_verification' 'calculated_host_listings_count'\n",
      "  'reviews_per_month']\n",
      " ['12147973' '31303940' '1' '1' '02131' '42.282618795779484'\n",
      "  '-71.13306792912681' 1 3 2 '4' '1.5' '2' '3' 3 None 250.0 None None\n",
      "  None 35.0 '1' 0.0 '2' '1125' 10 '0' '0' '0' '0' '0' None None None None\n",
      "  None None None 0 0 1 0 0 '1' None]\n",
      " ['3075044' '2572247' '1' '1' '02131' '42.286240821867416'\n",
      "  '-71.13437396457161' 1 12 0 '2' '1.0' '1' '1' 3 None 65.0 400.0 None\n",
      "  95.0 10.0 '0' 0.0 '2' '15' 8 '26' '54' '84' '359' '36' '94' '10' '9'\n",
      "  '10' '10' '9' '9' 0 1 1 0 0 '1' '1.30']\n",
      " ['6976' '16701' '1' '1' '02131' '42.2924378866568' '-71.13576525374667'\n",
      "  1 12 0 '2' '1.0' '1' '1' 3 None 65.0 395.0 1350.0 None None '1' 20.0\n",
      "  '3' '45' 6 '19' '46' '61' '319' '41' '98' '10' '9' '10' '10' '9' '10' 0\n",
      "  0 1 1 0 '1' '0.47']\n",
      " ['1436513' '6031442' '1' '1' None '42.28110618827366'\n",
      "  '-71.12102117350553' 0 3 0 '4' '1.0' '1' '2' 3 None 75.0 None None\n",
      "  100.0 50.0 '2' 25.0 '1' '1125' 8 '6' '16' '26' '98' '1' '100' '10' '10'\n",
      "  '10' '10' '10' '10' 0 0 1 0 0 '1' '1']]\n",
      "neighborhoods data \n",
      " ['neighbourhood' 'Roslindale' 'Roslindale' ... 'Charlestown' 'Somerville'\n",
      " 'Somerville']\n"
     ]
    }
   ],
   "source": [
    "# listings = np.loadtxt('archive/listings.csv', delimiter=',', skiprows = 1)\n",
    "'''\n",
    "but leaving out rows that dont have a value for host_neighborhood  and/or neighborhood \n",
    "\n",
    "'''\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "listings_csvfile = open('archive/listings.csv', 'r') \n",
    "reader = csv.reader(listings_csvfile, delimiter=',', quotechar='\"')\n",
    "listings_data = []\n",
    "column = None \n",
    "property_types = {'': 0, 'Condominium': 1, 'Camper/RV': 2, 'House': 3, 'Townhouse': 4, 'Entire Floor': 5, \n",
    "              'Guesthouse': 6, 'Boat': 7, 'Dorm': 8, 'Villa': 9, 'Bed & Breakfast': 10, 'Other': 11, 'Apartment': 12, \n",
    "              'Loft': 13 }\n",
    "\n",
    "room_types = {'Private room': 0, 'Shared room': 1, 'Entire home/apt': 2}\n",
    "beds = {'Couch': 0, 'Airbed':1, 'Pull-out Sofa':2, 'Real Bed':3, 'Futon':4}\n",
    "calendar_updated = {'never': 0, 'yesterday': 1, 'today': 2, '2 days ago': 3, '3 days ago': 4,\n",
    "                    '4 days ago': 5, '5 days ago': 6, '6 days ago': 7, 'a week ago': 8, '1 week ago': 9,\n",
    "                    '2 weeks ago': 10, '3 weeks ago': 11,'4 weeks ago': 12, '5 weeks ago': 13, '6 weeks ago': 14, \n",
    "                    '7 weeks ago': 15,'2 months ago': 16,'3 months ago': 17,'4 months ago': 18,'5 months ago': 19,\n",
    "                    '7 months ago': 20,'6 months ago': 21, '8 months ago': 22,'9 months ago': 23,'10 months ago':24,\n",
    "                    '11 months ago':25,'12 months ago':26, '13 months ago': 27, '14 months ago': 28, '15 months ago': 29, '16 months ago': 30, \n",
    "                    '17 months ago': 31, '18 months ago': 32, '20 months ago': 33, '22 months ago': 34, '30 months ago': 35, \n",
    "                    '23 months ago': 36, '25 months ago': 37 }\n",
    "cancellation_policy = {'flexible': 0, 'moderate': 1, 'strict': 2, 'super_strict_30': 3}\n",
    "indicesSet = set([0,19,32,33,43,48,49,53,54,55,56,59,65, \\\n",
    "                  67,68,71,72,73,74,76,79,80,81,82,83,84,85,93,94,50,86,89,91,92,51,52,57,\\\n",
    "                 60,61,62,63,64,66,69,90]) #columns included in the numpy array \n",
    "neighborhoods = [ ]\n",
    "for index, row in enumerate(reader): \n",
    "    #removing rows without a listed neighborhood in either of these 4 columns that correspond to neighborhood\n",
    "    host_neighbourhood = row[31]\n",
    "    neighbourhood = row[38]\n",
    "    neighbourhood_cleansed = row[39]\n",
    "    neighbourhood_group_cleansed = row[40]\n",
    "\n",
    "    if not neighbourhood and not host_neighbourhood \\\n",
    "        and not neighbourhood_cleansed and not neighbourhood_group_cleansed:\n",
    "        continue \n",
    "    #adding neighborhood from whatever column it's stored in\n",
    "    if neighbourhood: \n",
    "        neighborhoods.append(neighbourhood)\n",
    "    elif host_neighbourhood:\n",
    "        neighborhoods.append(host_neighbourhood)\n",
    "    elif neighbourhood_cleansed:\n",
    "        neighborhoods.append(neighbourhood_cleansed)\n",
    "    elif neighbourhood_group_cleansed:\n",
    "        neighborhoods.append(neighbourhood_group_cleansed)\n",
    "        \n",
    "        \n",
    "\n",
    "    if index == 0:\n",
    "        columns = row\n",
    "    r  = [ ] #row values\n",
    "    for i in range(95): #i is a column index \n",
    "        if index == 0 and i in indicesSet: #then this is the header column names\n",
    "            r.append(row[i])\n",
    "        elif i == 0 or i == 19 or i == 32 or i == 33 or i == 43 \\\n",
    "            or i == 48 or i == 49 or i == 53 or i == 54 or i == 55 or i == 56 or i == 59 \\\n",
    "            or i == 65 or i == 67 or i == 68 or (i >=71 and i <= 74) or i == 76 or (i >= 79 \\\n",
    "            and i <= 85) or i ==  93 or i == 94: #listing id, host id \n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                val = row[i] \n",
    "                r.append(val)\n",
    "        elif i == 50 or i == 86 or i == 89 or i == 91 or i == 92:\n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                boolean = 0 if row[i] == \"f\" else 1 \n",
    "                r.append(boolean)\n",
    "        elif i == 51:\n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                property_type = property_types[row[i]]\n",
    "                r.append(property_type)\n",
    "        elif i == 52:\n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                room_type = room_types[row[i]] \n",
    "                r.append(room_type)\n",
    "        elif i == 57:\n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                bed = row[i]\n",
    "                r.append(beds[bed])\n",
    "        elif (i >=  60 and i <= 64) or i == 66: \n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                money = row[i]\n",
    "                value = float(Decimal(sub(r'[^\\d.]', '', money)))#remove dollar sign, comma, and everything after the .\n",
    "                r.append(value)\n",
    "        elif i == 69: #calendar_updated col \n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                date = row[i]\n",
    "                r.append(calendar_updated[date])  \n",
    "        elif i == 90:\n",
    "            if row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "            else:\n",
    "                policy = row[i]\n",
    "                val = cancellation_policy[(policy)] \n",
    "                r.append(val)\n",
    "        \n",
    "    listings_data.append(r)\n",
    "\n",
    "listings_numerical_data = np.array(listings_data)\n",
    "neighborhoods = np.array(neighborhoods)\n",
    "# print(columns)\n",
    "print(\"listings data \\n\", listings_numerical_data[:5,:]) #first 5 rows\n",
    "print(\"neighborhoods data \\n\", neighborhoods )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7f04a",
   "metadata": {},
   "source": [
    "\n",
    "Christine's to-dos:\n",
    "-  remove stuff internally used\n",
    "-  remove urls \n",
    "- leave dates in date form \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31739a",
   "metadata": {},
   "source": [
    "Text Analysis\n",
    "- Remove neighborhood names from text-based feature columns used for training\n",
    "- Bag of words: tfid vectorization\n",
    "- Link reviews to correct neighborhood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85175f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68275\n",
      "68275\n",
      "68275\n",
      "51206\n",
      "51206\n",
      "17069\n",
      "17069\n"
     ]
    }
   ],
   "source": [
    "#Here is where Jiawei and Sara will do text analysis\n",
    "# split data by neighborhood\n",
    "\n",
    "def readSpamData(ham_filename, spam_filename):\n",
    "    DATA = []\n",
    "\n",
    "    # Read in ham messages from file\n",
    "    ham_file = open(ham_filename, 'r')\n",
    "    messages = ham_file.read().split('*****@@@@@%%%%%*****@@@@@%%%%%')\n",
    "    for message in messages: DATA.append([message, 0])\n",
    "    ham_file.close()\n",
    "    \n",
    "    # Read in spam messages from file\n",
    "    spam_file = open(spam_filename, 'r')\n",
    "    messages = spam_file.read().split('*****@@@@@%%%%%*****@@@@@%%%%%')\n",
    "    for message in messages: DATA.append([message, 1])\n",
    "    spam_file.close()\n",
    "\n",
    "    random.shuffle(DATA)  # Shuffle\n",
    "    return [row[0] for row in DATA], [row[1] for row in DATA]\n",
    "\n",
    "# corpus, labels = readSpamData('ham.txt', 'spam.txt')\n",
    "corpus = reviews[:,5]\n",
    "labels = reviews[:,1]\n",
    "print(len(corpus))\n",
    "print(len(labels))\n",
    "print(len(reviews))\n",
    "# print(reviews[:,5])\n",
    "# Separate into training and testing data\n",
    "TEST_SIZE = 0.25  \n",
    "# MAKE SURE ITS SPLIT BEFORE USING!!!!!\n",
    "\n",
    "separator = int((1.0 - TEST_SIZE)*len(corpus))\n",
    "corpus_train = corpus[:separator]\n",
    "labels_train = labels[:separator]\n",
    "corpus_test = corpus[separator:]\n",
    "labels_test = labels[separator:]\n",
    "print(len(corpus_train))\n",
    "print(len(labels_train))\n",
    "print(len(corpus_test))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28100a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51206, 41894)\n",
      "(51206,)\n"
     ]
    }
   ],
   "source": [
    "# Text feature extraction for training data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()  # Use individual words as tokens\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "y_train = np.array(labels_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70bf6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17069, 41894)\n",
      "(17069,)\n"
     ]
    }
   ],
   "source": [
    "# Text feature extraction for testing data\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "y_test = np.array(labels_test)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40338f65",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b71ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is where Christine will feature scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d4c41",
   "metadata": {},
   "source": [
    "split into training and testing\n",
    "- first organize stuff by neighborhood\n",
    "- randomly permute rows of each neighborhood set\n",
    "- then select 25% of each neighborhood for testing and the rest for training\n",
    "- combine the testing components and the training components\n",
    "\n",
    "- **** we also planned to do cross validation, but maybe we can revisit that later?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94234a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-89e0cfef8085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Use feature scaling to standardize the features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscaleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#Here is where Christine will split data and randomly permute rows\n",
    "\n",
    "# EX. copied from previous exercise\n",
    "\n",
    "# Split data into training (80%) and testing (20%).\n",
    "# Use feature scaling to standardize the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "def scaleData(trainX, testX):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trainX)\n",
    "    return scaler.transform(trainX), scaler.transform(testX)\n",
    "\n",
    "X_train, X_test = scaleData(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3498c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814a489",
   "metadata": {},
   "source": [
    "Create Classification models\n",
    "- kNN\n",
    "- random forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Neural Network (or Perceptron)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6714650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of kNN:\t0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model  # Using sklearn Perceptron classifier\n",
    "from sklearn import ensemble  # Using RandomForest classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import neighbors  # Using nearest neighbors classifier\n",
    "# learners = {'Perceptron': linear_model.Perceptron(max_iter=10),\n",
    "#             'RandomForest': ensemble.RandomForestClassifier(),\n",
    "#             'kNN': neighbors.KNeighborsClassifier(),\n",
    "#             'SVM': SVC(),\n",
    "#             'LogisticRegression': linear_model.LogisticRegression(random_state=42)\n",
    "#            },\n",
    "           }\n",
    "# for classifierName in learners:\n",
    "#     learners[classifierName].fit(X_train, y_train)\n",
    "#     print('Accuracy of ' + classifierName + ':\\t' + str(learners[classifierName].score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bdd02a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e85055014abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m  \u001b[0;31m# Get the learned Perceptron weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msorted_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "#Here is where Jiawei will do NN\n",
    "# Get Perceptron weights\n",
    "# learner = linear_model.Perceptron(max_iter=1)\n",
    "learner.fit(X_train, y_train)\n",
    "weights = learner.coef_  # Get the learned Perceptron weights\n",
    "sorted_weights = np.argsort(weights)  # Sort\n",
    "features = vectorizer.get_feature_names()  # Get the features\n",
    "print('\\nLowest weighted words (indicative of ham)')\n",
    "for i in range(5): print(features[sorted_weights[0,i]])\n",
    "print('\\nHighest weighted words (indicative of spam)')\n",
    "for i in range(5): print(features[sorted_weights[0,len(sorted_weights[0])-i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f33b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
