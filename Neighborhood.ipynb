{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dcf29f",
   "metadata": {},
   "source": [
    "City Neighborhood Classifier \n",
    "\n",
    "CS 305: Machine Learning Fall 2021\n",
    "\n",
    "Sara Clark, Christine Pourheydarian, Jiawei Liu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1771af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: six>=1.10 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: urllib3 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.26.4)\n",
      "Requirement already satisfied: python-slugify in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: tqdm in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (4.59.0)\n",
      "Requirement already satisfied: certifi in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: requests in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.25.1)\n",
      "Requirement already satisfied: python-dateutil in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/saraclark/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ef147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54351311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20170905' 0 None 12147973]\n",
      " ['20170904' 0 None 12147973]\n",
      " ['20170903' 0 None 12147973]\n",
      " ...\n",
      " ['20160908' 0 None 14504422]\n",
      " ['20160907' 0 None 14504422]\n",
      " ['20160906' 0 None 14504422]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "importing the calendar.csv data. \n",
    "listing id stays a number\n",
    "date is kept in its form but has the dashes removed\n",
    "available: if f, then becomes 0. if t, then becomes 1. \n",
    "price: converted from currency to a double. if there is no price, the value inserted is None. \n",
    "'''\n",
    "\n",
    "\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "calendar_csvfile = open('archive/calendar.csv', 'r')#listing_id,date,available,price\n",
    "reader = csv.reader(calendar_csvfile, delimiter=',', quotechar='\"')\n",
    "calendar_data = []\n",
    "\n",
    "for row in reader: \n",
    "    r  = [ ]\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            date = (row[1]).replace(\"-\", \"\")\n",
    "            r.append(date)\n",
    "        elif i == 1:\n",
    "            available = 0 if row[2] == \"f\" else 1 #f: 0, t: 1 \n",
    "            r.append(available)\n",
    "        elif i == 2:        \n",
    "            money = row[3]\n",
    "            if money == \"\":\n",
    "                r.append(None)\n",
    "                continue\n",
    "            else: \n",
    "                value = float(Decimal(sub(r'[^\\d.]', '', money)))#remove dollar sign, comma, and everything after the .\n",
    "                r.append(value)\n",
    "        elif i == 3: \n",
    "            listing_id  = int(row[0])\n",
    "            r.append(listing_id)    \n",
    "    calendar_data.append(r)\n",
    "\n",
    "\n",
    "calendar_data_np = np.array(calendar_data)\n",
    "print(calendar_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "240f7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1178162' '4724140' '2013-05-21' '4298113' 'Olivier'\n",
      " \"My stay at islam's place was really cool! Good location, 5min away from subway, then 10min from downtown. The room was nice, all place was clean. Islam managed pretty well our arrival, even if it was last minute ;) i do recommand this place to any airbnb user :)\"]\n",
      "[\"My stay at islam's place was really cool! Good location, 5min away from subway, then 10min from downtown. The room was nice, all place was clean. Islam managed pretty well our arrival, even if it was last minute ;) i do recommand this place to any airbnb user :)\"\n",
      " 'Great location for both airport and city - great amenities in the house: Plus Islam was always very helpful even though he was away'\n",
      " \"We really enjoyed our stay at Islams house. From the outside the house didn't look so inviting but the inside was very nice! Even though Islam himself was not there everything was prepared for our arrival. The airport T Station is only a 5-10 min walk away. The only little issue was that all the people in the house had to share one bathroom. But it was not really a problem and it worked out fine. We would recommend Islams place for a stay in Boston. \"\n",
      " ...\n",
      " 'The room was very clean as were the bathrooms and kitchen. The bed was also very comfortable. Bathrooms featured shampoo and body was for public use which was a plus along with good wifi and air conditioning. The neighborhood was safe and populated with many students. Conveniently located about a mile from Harvard and Boston University. Many great places too eat are also a 10~ minute walk from the room. Joe was also very helpful in returning an earring to me that I had left in the room. Would definitely stay again.'\n",
      " \"Staying in Lower Allston at Joe and Nancy's place was fantastic! I am a medical student and was studying at the Massachusetts General Hospital for 6 weeks so it was really important for me to find a place that was clean, private, comfortable, safe and friendly with easy access to Harvard Square/ the red line/ the hospital. Joe's place provided all of this and more! \\n\\nThe room was spacious and clean with air conditioning and a heater. I had easy access to all the facilities I needed including a fully equipped kitchen, laundry facilities and 2 bathrooms. Joe went above and beyond to make sure I had everything I needed and went out and bought an ironing board when I asked if one was available for me to iron my shirts with! All the guests that stayed during my 6 weeks were also respectful and friendly which made my trip even better. \\n\\nThe house is in a pleasant residential area which is quiet at night time but close to all important amenities. The nearest supermarket was only 10 minutes walk away and the number 66 bus route had a stop at the top of the road and had a regular service every 5-10 minutes which took me into Harvard Square within 10 minutes. \\n\\nBetween my excellent hosts and the fantastic location I felt completely at home during my stay and would recommend Joe's place to anyone looking visit Boston without paying the ridiculous price of downtown hotels. It was an absolute pleasure to get to know him and his wife Nancy and I hope they stay in touch in the future. I wish you both all the best! \"\n",
      " \"The room itself and the aprtment were very clean. Joe provided many amenities such as shampoo and towels. Overall I'm very pleased!\"]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loading reviews.cvs into numpy w/o any modification, i.e. w/o switching anything from qualitative to quantitative  \n",
    "'''\n",
    "\n",
    "csv_file = open('archive/reviews.csv', 'r')\n",
    "reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "data = []\n",
    "for i, row in enumerate(reader):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    r = []\n",
    "    for i in range(6):\n",
    "        r.append(row[i])\n",
    "    data.append(r)\n",
    "\n",
    "csv_file.close()\n",
    "reviews = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1129c6e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12147973', None, None, '31303940', None, None, None, '1', '1', None, '02131', '42.282618795779484', '-71.13306792912681', 't', 'House', 'Entire home/apt', '4', '1.5', '2', '3', 'Real Bed', '', '$250.00', '', '', '', '$35.00', '1', '$0.00', '2', '1125', '2 weeks ago', None, '0', '0', '0', '0', '0', None, None, '', '', '', '', '', '', '', 'f', None, None, 'f', 'moderate', 'f', 'f', '1', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-e3ecef8ceed4>:127: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  listings_numerical_data = np.array(listings_data)\n"
     ]
    }
   ],
   "source": [
    "# listings = np.loadtxt('archive/listings.csv', delimiter=',', skiprows = 1)\n",
    "'''\n",
    "but leaving out rows that dont have a value for host_neighborhood  and/or neighborhood \n",
    "\n",
    "'''\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "listings_csvfile = open('archive/listings.csv', 'r') \n",
    "reader = csv.reader(listings_csvfile, delimiter=',', quotechar='\"')\n",
    "listings_data = []\n",
    "column = None \n",
    "property_types = {'': 0, \n",
    "              'Condominium': 1, \n",
    "              'Camper/RV': 2, \n",
    "              'House': 3, \n",
    "              'Townhouse': 4, \n",
    "              'Entire Floor': 5, \n",
    "              'Guesthouse': 6, \n",
    "              'Boat': 7, \n",
    "              'Dorm': 8, \n",
    "              'Villa': 9, \n",
    "              'Bed & Breakfast': 10, \n",
    "              'Other': 11, \n",
    "              'Apartment': 12, \n",
    "              'Loft': 13 \n",
    "             }\n",
    "\n",
    "room_types = {'Private room': 0, 'Shared room': 1, 'Entire home/apt': 2}\n",
    "beds = {'Couch': 0, 'Airbed':1, 'Pull-out Sofa':2, 'Real Bed':3, 'Futon':4}\n",
    "calendar_updated = {'never': 0,\n",
    "                    'yesterday': 1, \n",
    "                    'today': 2,\n",
    "                    '2 days ago': 3,\n",
    "                    '3 days ago': 4,\n",
    "                    '4 days ago': 5, \n",
    "                    '5 days ago': 6, \n",
    "                    '6 days ago': 7, \n",
    "                    'a week ago': 8,\n",
    "                    '1 week ago': 9,\n",
    "                    '2 weeks ago': 10, \n",
    "                    '3 weeks ago': 11,\n",
    "                    '4 weeks ago': 12, \n",
    "                    '5 weeks ago': 13, \n",
    "                    '6 weeks ago': 14, \n",
    "                    '7 weeks ago': 15,\n",
    "                    '2 months ago': 16,\n",
    "                    '3 months ago': 17,\n",
    "                    '4 months ago': 18,\n",
    "                    '5 months ago': 19,\n",
    "                    '7 months ago': 20,\n",
    "                    '6 months ago': 21, \n",
    "                    '8 months ago': 22,\n",
    "                    '9 months ago': 23,\n",
    "                    '10 months ago':24,\n",
    "                    '11 months ago':25,\n",
    "                    '12 months ago':26, \n",
    "                    '13 months ago': 27, \n",
    "                    '14 months ago': 28, \n",
    "                    '15 months ago': 29, \n",
    "                    '16 months ago': 30, \n",
    "                    '17 months ago': 31, \n",
    "                    '18 months ago': 32, \n",
    "                    '20 months ago': 33, \n",
    "                    '22 months ago': 34, \n",
    "                    '30 months ago': 35, \n",
    "                    '23 months ago': 36, \n",
    "                    '25 months ago': 37 \n",
    "             }\n",
    "cancellation_policy = {'flexible': 0, \n",
    "                       'moderate': 1, \n",
    "                       'strict': 2, \n",
    "                       'super_strict_30': 3}\n",
    "indicesSet = set([0,19,32,33,43,48,49,53,54,55,56,59,65, \\\n",
    "                  67,68,71,72,73,74,76,79,80,81,82,83,84,85,93,94,50,86,89,91,92,51,52,57,\\\n",
    "                 60,61,62,63,64,66,69,90])\n",
    "for index, row in enumerate(reader):\n",
    "        \n",
    "    if index == 0:\n",
    "        columns = row\n",
    "    r  = [ ]\n",
    "    for i in range(95):\n",
    "        if i in indicesSet:\n",
    "            r.append(row[i])\n",
    "        elif row[i] == \"N/A\" or row[i] == \"\":\n",
    "                r.append(None)\n",
    "                continue \n",
    "        elif i == 0 or i == 19 or i == 32 or i == 33 or i == 43 \\\n",
    "        or i == 48 or i == 49 or i == 53 or i == 54 or i == 55 or i == 56 or i == 59 \\\n",
    "        or i == 65 or i == 67 or i == 68 or (i >=71 and i <= 74) or i == 76 or (i >= 79 \\\n",
    "        and i <= 85) or i ==  93 or i == 94: #listing id, host id \n",
    "            val = row[i] \n",
    "            r.append(val)\n",
    "        elif i == 50 or i == 86 or i == 89 or i == 91 or i == 92:\n",
    "            boolean = 0 if row[i] == \"f\" else 1 \n",
    "            r.append(boolean)\n",
    "        elif i == 51: \n",
    "            property_type = property_types[row[i]]\n",
    "            r.append(property_type)\n",
    "        elif i == 52:\n",
    "            room_type = room_types[row[i]] \n",
    "            r.append(room_type)\n",
    "        elif i == 57:\n",
    "            bed = row[i]\n",
    "            r.append(beds[bed])\n",
    "        elif (i >=  60 and i <= 64) or i == 66: \n",
    "            money = row[i]\n",
    "            if money == \"\":\n",
    "                r.append(None)\n",
    "                continue\n",
    "            else: \n",
    "                value = float(Decimal(sub(r'[^\\d.]', '', money)))#remove dollar sign, comma, and everything after the .\n",
    "                r.append(value)\n",
    "        elif i == 69: #calendar_updated col \n",
    "            date = row[i]\n",
    "            r.append(calendar_updated[date])  \n",
    "        elif i == 90:\n",
    "            policy = row[i]\n",
    "            val = cancellation_policy[(policy)] \n",
    "            r.append(val)\n",
    "        \n",
    "    listings_data.append(r)\n",
    "\n",
    "# print(columns[91])\n",
    "\n",
    "listings_numerical_data = np.array(listings_data)\n",
    "print(listings_numerical_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7f04a",
   "metadata": {},
   "source": [
    "\n",
    "Christine's to-dos:\n",
    "-  remove stuff internally used\n",
    "-  remove urls \n",
    "-  None for all empty values\n",
    "- leave dates in date form \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31739a",
   "metadata": {},
   "source": [
    "Text Analysis\n",
    "- Remove neighborhood names from text-based feature columns used for training\n",
    "- Bag of words: tfid vectorization\n",
    "- Link reviews to correct neighborhood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85175f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68275\n",
      "68275\n",
      "68275\n",
      "51206\n",
      "51206\n",
      "17069\n",
      "17069\n"
     ]
    }
   ],
   "source": [
    "#Here is where Jiawei and Sara will do text analysis\n",
    "# split data by neighborhood\n",
    "\n",
    "def readSpamData(ham_filename, spam_filename):\n",
    "    DATA = []\n",
    "\n",
    "    # Read in ham messages from file\n",
    "    ham_file = open(ham_filename, 'r')\n",
    "    messages = ham_file.read().split('*****@@@@@%%%%%*****@@@@@%%%%%')\n",
    "    for message in messages: DATA.append([message, 0])\n",
    "    ham_file.close()\n",
    "    \n",
    "    # Read in spam messages from file\n",
    "    spam_file = open(spam_filename, 'r')\n",
    "    messages = spam_file.read().split('*****@@@@@%%%%%*****@@@@@%%%%%')\n",
    "    for message in messages: DATA.append([message, 1])\n",
    "    spam_file.close()\n",
    "\n",
    "    random.shuffle(DATA)  # Shuffle\n",
    "    return [row[0] for row in DATA], [row[1] for row in DATA]\n",
    "\n",
    "# corpus, labels = readSpamData('ham.txt', 'spam.txt')\n",
    "corpus = reviews[:,5]\n",
    "labels = reviews[:,1]\n",
    "print(len(corpus))\n",
    "print(len(labels))\n",
    "print(len(reviews))\n",
    "# print(reviews[:,5])\n",
    "# Separate into training and testing data\n",
    "TEST_SIZE = 0.25  \n",
    "# MAKE SURE ITS SPLIT BEFORE USING!!!!!\n",
    "\n",
    "separator = int((1.0 - TEST_SIZE)*len(corpus))\n",
    "corpus_train = corpus[:separator]\n",
    "labels_train = labels[:separator]\n",
    "corpus_test = corpus[separator:]\n",
    "labels_test = labels[separator:]\n",
    "print(len(corpus_train))\n",
    "print(len(labels_train))\n",
    "print(len(corpus_test))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28100a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51206, 41894)\n",
      "(51206,)\n",
      "  (0, 16004)\t0.18159843451113022\n",
      "  (0, 33203)\t0.2567278598437474\n",
      "  (0, 16131)\t0.18028549566396285\n",
      "  (0, 35386)\t0.09542250388673663\n",
      "  (0, 1999)\t0.24428367190544095\n",
      "  (0, 25256)\t0.283466098191063\n",
      "  (0, 16691)\t0.16718873142405255\n",
      "  (0, 17256)\t0.09115961611960623\n",
      "  (0, 2072)\t0.2673801565852983\n",
      "  (0, 6697)\t0.2042350834151937\n",
      "  (0, 2199)\t0.06847169650435873\n",
      "  (0, 1678)\t0.2379027799362711\n",
      "  (0, 5000)\t0.2555321494561598\n",
      "  (0, 13797)\t0.10767348112623298\n",
      "  (0, 15324)\t0.21356723825201737\n",
      "  (0, 12394)\t0.19440614825535793\n",
      "  (0, 33096)\t0.07198351557737255\n",
      "  (0, 3592)\t0.2129956590189964\n",
      "  (0, 20054)\t0.12628185462711838\n",
      "  (0, 36110)\t0.1594374640530305\n",
      "  (0, 18077)\t0.5064715447060048\n"
     ]
    }
   ],
   "source": [
    "# Text feature extraction for training data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()  # Use individual words as tokens\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "y_train = np.array(labels_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b70bf6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17069, 41894)\n",
      "(17069,)\n"
     ]
    }
   ],
   "source": [
    "# Text feature extraction for testing data\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "y_test = np.array(labels_test)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a309dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40338f65",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b71ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is where Christine will feature scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d4c41",
   "metadata": {},
   "source": [
    "split into training and testing\n",
    "- first organize stuff by neighborhood\n",
    "- randomly permute rows of each neighborhood set\n",
    "- then select 25% of each neighborhood for testing and the rest for training\n",
    "- combine the testing components and the training components\n",
    "\n",
    "- **** we also planned to do cross validation, but maybe we can revisit that later?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94234a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-89e0cfef8085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Use feature scaling to standardize the features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscaleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#Here is where Christine will split data and randomly permute rows\n",
    "\n",
    "# EX. copied from previous exercise\n",
    "\n",
    "# Split data into training (80%) and testing (20%).\n",
    "# Use feature scaling to standardize the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "def scaleData(trainX, testX):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trainX)\n",
    "    return scaler.transform(trainX), scaler.transform(testX)\n",
    "\n",
    "X_train, X_test = scaleData(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3498c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814a489",
   "metadata": {},
   "source": [
    "Create Classification models\n",
    "- kNN\n",
    "- random forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Neural Network (or Perceptron)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6714650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of kNN:\t0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model  # Using sklearn Perceptron classifier\n",
    "from sklearn import ensemble  # Using RandomForest classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import neighbors  # Using nearest neighbors classifier\n",
    "# learners = {'Perceptron': linear_model.Perceptron(max_iter=10),\n",
    "#             'RandomForest': ensemble.RandomForestClassifier(),\n",
    "#             'kNN': neighbors.KNeighborsClassifier(),\n",
    "#             'SVM': SVC(),\n",
    "#             'LogisticRegression': linear_model.LogisticRegression(random_state=42)\n",
    "#            },\n",
    "           }\n",
    "# for classifierName in learners:\n",
    "#     learners[classifierName].fit(X_train, y_train)\n",
    "#     print('Accuracy of ' + classifierName + ':\\t' + str(learners[classifierName].score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bdd02a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e85055014abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m  \u001b[0;31m# Get the learned Perceptron weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msorted_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "#Here is where Jiawei will do NN\n",
    "# Get Perceptron weights\n",
    "# learner = linear_model.Perceptron(max_iter=1)\n",
    "learner.fit(X_train, y_train)\n",
    "weights = learner.coef_  # Get the learned Perceptron weights\n",
    "sorted_weights = np.argsort(weights)  # Sort\n",
    "features = vectorizer.get_feature_names()  # Get the features\n",
    "print('\\nLowest weighted words (indicative of ham)')\n",
    "for i in range(5): print(features[sorted_weights[0,i]])\n",
    "print('\\nHighest weighted words (indicative of spam)')\n",
    "for i in range(5): print(features[sorted_weights[0,len(sorted_weights[0])-i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f33b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
